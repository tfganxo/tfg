{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28182ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "import mglearn\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Random global\n",
    "seed = 12345\n",
    "rng = np.random.default_rng(seed)  # can be called without a seed\n",
    "rng.random()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv (r\"C:\\Users\\White\\Desktop\\tfg\\generados\\RFECved.csv\")\n",
    "dfNon = pd.read_csv (r\"C:\\Users\\White\\Desktop\\tfg\\generados\\scaled.csv\")\n",
    "df2 = pd.read_csv (r\"C:\\Users\\White\\Desktop\\tfg\\generados\\Deoutliers2.csv\")\n",
    "dfR = pd.read_csv (r\"C:\\Users\\White\\Desktop\\tfg\\generados\\Raw-ishDf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5541837",
   "metadata": {},
   "source": [
    "# GaussianNB as a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d904c8",
   "metadata": {},
   "source": [
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee837515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'var_smoothing': 1e-08}\n",
      "0.9864184815803458\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'var_smoothing': 1e-08}\n",
      "0.986673067244306\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'var_smoothing': 1e-08}\n",
      "0.9863698248252482\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'var_smoothing': 1e-05}\n",
      "0.9841536627722809\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'var_smoothing': 1e-08}\n",
      "0.9847713463556648\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "dfInterno=pd.DataFrame()\n",
    "dfBest=pd.DataFrame()\n",
    "\n",
    "#baseline parameters\n",
    "p_grid = { \n",
    "    'var_smoothing': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "}\n",
    "\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    grid = GridSearchCV(GaussianNB(),return_train_score=True,scoring=scoring, refit='accuracy', param_grid=p_grid, n_jobs=-1, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=3)\n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21d0ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\GaussianGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08788c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     45521\n",
      "           1       1.00      0.99      0.99    204911\n",
      "\n",
      "    accuracy                           0.99    250432\n",
      "   macro avg       0.97      0.99      0.98    250432\n",
      "weighted avg       0.99      0.99      0.99    250432\n",
      "\n",
      "0.9881045553283926\n",
      "Precision Score :  0.9881045553283926\n",
      "Recall Score :  0.9881045553283926\n",
      "0.9881045553283926\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     15174\n",
      "           1       1.00      0.99      0.99     68304\n",
      "\n",
      "    accuracy                           0.99     83478\n",
      "   macro avg       0.97      0.99      0.98     83478\n",
      "weighted avg       0.99      0.99      0.99     83478\n",
      "\n",
      "0.9879010038573037\n",
      "Precision Score :  0.9879010038573037\n",
      "Recall Score :  0.9879010038573037\n",
      "0.9879010038573037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Results tested on the whole dataset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "##############################################################\n",
    "gnb = GaussianNB(var_smoothing=1e-08)\n",
    "pipe= make_pipeline(MinMaxScaler(),StandardScaler(),gnb)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(f1_score(y_train,y_pred,average='micro'))\n",
    "#################################################################\n",
    "y_pred = pipe.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(f1_score(y_test,y_pred,average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b692e6",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "315bd770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'max_depth': 5, 'n_estimators': 200}\n",
      "0.999363601040338\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'max_depth': 5, 'n_estimators': 200}\n",
      "0.9994085225680325\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'max_depth': 5, 'n_estimators': 100}\n",
      "0.9994010361372409\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'max_depth': 5, 'n_estimators': 200}\n",
      "0.9994197534754818\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'max_depth': 5, 'n_estimators': 200}\n",
      "0.9993823190792801\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "dfInterno=pd.DataFrame()\n",
    "# define the model with default hyperparameters\n",
    "model=RandomForestClassifier(random_state=42)\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "# define the grid of values to search\n",
    "p_grid = { \n",
    "    'n_estimators': [100, 200],\n",
    "    #Empirical good default values [...] max_features=sqrt(n_features) for classification tasks\n",
    "   # 'max_features': [\"auto\", \"sqrt\"],\n",
    "    'max_depth' : [3,4,5],\n",
    "    #'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=-1, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0caeff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\RForestGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1c89881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45521\n",
      "           1       1.00      1.00      1.00    204911\n",
      "\n",
      "    accuracy                           1.00    250432\n",
      "   macro avg       1.00      1.00      1.00    250432\n",
      "weighted avg       1.00      1.00      1.00    250432\n",
      "\n",
      "0.9994090212113468\n",
      "Precision Score :  0.9994090212113468\n",
      "Recall Score :  0.9994090212113468\n",
      "0.9994090212113468\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15174\n",
      "           1       1.00      1.00      1.00     68304\n",
      "\n",
      "    accuracy                           1.00     83478\n",
      "   macro avg       1.00      1.00      1.00     83478\n",
      "weighted avg       1.00      1.00      1.00     83478\n",
      "\n",
      "0.999329164570306\n",
      "Precision Score :  0.999329164570306\n",
      "Recall Score :  0.999329164570306\n",
      "0.999329164570306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Results tested on the whole dataset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "rfc=RandomForestClassifier(max_depth= 5, max_features='sqrt', n_estimators= 100, random_state=42)\n",
    "pipe= make_pipeline(MinMaxScaler(),StandardScaler(),rfc)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(f1_score(y_train,y_pred,average='micro'))\n",
    "#################################################################\n",
    "y_pred = pipe.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(f1_score(y_test,y_pred,average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60925434",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b040d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.9988395072312443\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.9987758670339766\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.9988170456966257\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.9987983276576836\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.9988170470279579\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "dfInterno=pd.DataFrame()\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# define the model with default hyperparameters\n",
    "model = AdaBoostClassifier()\n",
    "# define the grid of values to search\n",
    "p_grid = dict()\n",
    "p_grid['n_estimators'] = [100,200]\n",
    "p_grid['learning_rate'] = [0.1,0.001,0.0001]\n",
    "\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=-1, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17231836",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\AdaboostGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fe78832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45521\n",
      "           1       1.00      1.00      1.00    204911\n",
      "\n",
      "    accuracy                           1.00    250432\n",
      "   macro avg       1.00      1.00      1.00    250432\n",
      "weighted avg       1.00      1.00      1.00    250432\n",
      "\n",
      "0.9988180424226936\n",
      "Precision Score :  0.9988180424226936\n",
      "Recall Score :  0.9988180424226936\n",
      "0.9988180424226936\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15174\n",
      "           1       1.00      1.00      1.00     68304\n",
      "\n",
      "    accuracy                           1.00     83478\n",
      "   macro avg       1.00      1.00      1.00     83478\n",
      "weighted avg       1.00      1.00      1.00     83478\n",
      "\n",
      "0.9987421835693236\n",
      "Precision Score :  0.9987421835693236\n",
      "Recall Score :  0.9987421835693236\n",
      "0.9987421835693236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "adaB = AdaBoostClassifier(learning_rate= 0.1, n_estimators= 200)\n",
    "\n",
    "pipe= make_pipeline(MinMaxScaler(),StandardScaler(),adaB)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(f1_score(y_train,y_pred,average='micro'))\n",
    "#################################################################\n",
    "y_pred = pipe.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(f1_score(y_test,y_pred,average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9266a08",
   "metadata": {},
   "source": [
    "# LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70877f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'C': 1, 'tol': 0.0001}\n",
      "0.9988432513575514\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'C': 1, 'tol': 0.001}\n",
      "0.9989592997935859\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'C': 1, 'tol': 0.0001}\n",
      "0.9989705304908248\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'C': 1, 'tol': 0.0001}\n",
      "0.998951812101532\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'C': 1, 'tol': 0.0001}\n",
      "0.9988806872252256\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "dfInterno=pd.DataFrame()\n",
    "\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = LinearSVC()\n",
    "# define the grid of values to search\n",
    "p_grid = {\"tol\": [1e-3, 1e-4], \"C\": [1, 10, 100]}\n",
    "\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=2, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "811bb930",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\LinearSVCGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dfa68fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'loss', 'max_iter', 'multi_class', 'penalty', 'random_state', 'tol', 'verbose'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a80c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45521\n",
      "           1       1.00      1.00      1.00    204911\n",
      "\n",
      "    accuracy                           1.00    250432\n",
      "   macro avg       1.00      1.00      1.00    250432\n",
      "weighted avg       1.00      1.00      1.00    250432\n",
      "\n",
      "Accuracy Score :  0.9989617940199336\n",
      "Precision Score :  0.9989617940199336\n",
      "Recall Score :  0.9989617940199336\n",
      "F1 Score :  0.9989617940199336\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15174\n",
      "           1       1.00      1.00      1.00     68304\n",
      "\n",
      "    accuracy                           1.00     83478\n",
      "   macro avg       1.00      1.00      1.00     83478\n",
      "weighted avg       1.00      1.00      1.00     83478\n",
      "\n",
      "Accuracy Score :  0.9988979132226455\n",
      "Precision Score :  0.9988979132226455\n",
      "Recall Score :  0.9988979132226455\n",
      "f1 Score :  0.9988979132226455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "LSVC = LinearSVC(C= 1, tol= 0.0001)\n",
    "\n",
    "pipe= make_pipeline(MinMaxScaler(),StandardScaler(),LSVC)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"F1 Score : \",f1_score(y_train,y_pred,average='micro'))\n",
    "#################################################################\n",
    "y_pred = pipe.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"f1 Score : \",f1_score(y_test,y_pred,average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8279a7",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcc46bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/10\n",
      "1336/1336 [==============================] - 2s 1ms/step - loss: 0.3707 - accuracy: 0.9982\n",
      "Epoch 2/10\n",
      "1336/1336 [==============================] - 1s 1ms/step - loss: 0.1863 - accuracy: 0.9998\n",
      "Epoch 3/10\n",
      "1336/1336 [==============================] - 1s 986us/step - loss: 0.1200 - accuracy: 0.9999\n",
      "Epoch 4/10\n",
      "1336/1336 [==============================] - 1s 985us/step - loss: 0.0416 - accuracy: 0.9998\n",
      "Epoch 5/10\n",
      "1336/1336 [==============================] - 1s 988us/step - loss: 0.6765 - accuracy: 0.9998\n",
      "Epoch 6/10\n",
      "1336/1336 [==============================] - 1s 984us/step - loss: 0.2210 - accuracy: 0.9999\n",
      "Epoch 7/10\n",
      "1336/1336 [==============================] - 1s 985us/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1336/1336 [==============================] - 1s 994us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1336/1336 [==============================] - 1s 998us/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1336/1336 [==============================] - 1s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "{'batch_size': 200, 'epochs': 10, 'learning_rate': 0.2}\n",
      "0.9999775388858023\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/10\n",
      "2672/2672 [==============================] - 3s 924us/step - loss: 0.1584 - accuracy: 0.9987\n",
      "Epoch 2/10\n",
      "2672/2672 [==============================] - 2s 931us/step - loss: 0.0931 - accuracy: 0.9998\n",
      "Epoch 3/10\n",
      "2672/2672 [==============================] - 2s 919us/step - loss: 0.1092 - accuracy: 0.9999\n",
      "Epoch 4/10\n",
      "2672/2672 [==============================] - 2s 932us/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "2672/2672 [==============================] - 3s 939us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "2672/2672 [==============================] - 3s 954us/step - loss: 8.8480e-07 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "2672/2672 [==============================] - 3s 971us/step - loss: 1.6015e-07 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "2672/2672 [==============================] - 2s 929us/step - loss: 7.7626e-08 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "2672/2672 [==============================] - 2s 933us/step - loss: 2.6587e-08 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "2672/2672 [==============================] - 2s 931us/step - loss: 9.8852e-09 - accuracy: 1.0000\n",
      "{'batch_size': 100, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.999988769372831\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/10\n",
      "2672/2672 [==============================] - 3s 943us/step - loss: 0.1857 - accuracy: 0.9990\n",
      "Epoch 2/10\n",
      "2672/2672 [==============================] - 2s 930us/step - loss: 0.0985 - accuracy: 0.9998\n",
      "Epoch 3/10\n",
      "2672/2672 [==============================] - 2s 924us/step - loss: 0.1591 - accuracy: 0.9998\n",
      "Epoch 4/10\n",
      "2672/2672 [==============================] - 2s 927us/step - loss: 0.0912 - accuracy: 0.9999\n",
      "Epoch 5/10\n",
      "2672/2672 [==============================] - 2s 921us/step - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "2672/2672 [==============================] - 2s 921us/step - loss: 0.0191 - accuracy: 0.9999\n",
      "Epoch 7/10\n",
      "2672/2672 [==============================] - 2s 927us/step - loss: 0.2468 - accuracy: 0.9999\n",
      "Epoch 8/10\n",
      "2672/2672 [==============================] - 2s 924us/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "2672/2672 [==============================] - 2s 920us/step - loss: 0.2037 - accuracy: 0.9999\n",
      "Epoch 10/10\n",
      "2672/2672 [==============================] - 2s 922us/step - loss: 0.0749 - accuracy: 1.0000\n",
      "{'batch_size': 100, 'epochs': 10, 'learning_rate': 0.1}\n",
      "0.9999812825216186\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/10\n",
      "2672/2672 [==============================] - 3s 952us/step - loss: 1.4700 - accuracy: 0.9987\n",
      "Epoch 2/10\n",
      "2672/2672 [==============================] - 3s 977us/step - loss: 0.1346 - accuracy: 0.9998\n",
      "Epoch 3/10\n",
      "2672/2672 [==============================] - 2s 900us/step - loss: 0.1639 - accuracy: 0.9999\n",
      "Epoch 4/10\n",
      "2672/2672 [==============================] - 2s 892us/step - loss: 0.1631 - accuracy: 0.9999\n",
      "Epoch 5/10\n",
      "2672/2672 [==============================] - 2s 895us/step - loss: 0.3307 - accuracy: 0.9999\n",
      "Epoch 6/10\n",
      "2672/2672 [==============================] - 2s 894us/step - loss: 0.3988 - accuracy: 0.9999\n",
      "Epoch 7/10\n",
      "2672/2672 [==============================] - 2s 898us/step - loss: 0.1940 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "2672/2672 [==============================] - 2s 905us/step - loss: 0.4510 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "2672/2672 [==============================] - 3s 940us/step - loss: 0.1651 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "2672/2672 [==============================] - 2s 907us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "{'batch_size': 100, 'epochs': 10, 'learning_rate': 0.2}\n",
      "0.9999887695129711\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/10\n",
      "2672/2672 [==============================] - 3s 898us/step - loss: 0.5823 - accuracy: 0.9987\n",
      "Epoch 2/10\n",
      "2672/2672 [==============================] - 2s 883us/step - loss: 0.6313 - accuracy: 0.9998\n",
      "Epoch 3/10\n",
      "2672/2672 [==============================] - 2s 889us/step - loss: 0.3912 - accuracy: 0.9999\n",
      "Epoch 4/10\n",
      "2672/2672 [==============================] - 2s 884us/step - loss: 0.1571 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "2672/2672 [==============================] - 2s 889us/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "2672/2672 [==============================] - 2s 885us/step - loss: 0.9963 - accuracy: 0.9999\n",
      "Epoch 7/10\n",
      "2672/2672 [==============================] - 2s 849us/step - loss: 0.7647 - accuracy: 0.9999\n",
      "Epoch 8/10\n",
      "2672/2672 [==============================] - 2s 860us/step - loss: 0.1015 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "2672/2672 [==============================] - 2s 852us/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "2672/2672 [==============================] - 2s 856us/step - loss: 0.7185 - accuracy: 0.9999\n",
      "{'batch_size': 100, 'epochs': 10, 'learning_rate': 0.2}\n",
      "0.9999812824515486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "#Autoencoder\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learning_rate=0.01):\n",
    "    #Enc Dimension \n",
    "    encoding_dim=100\n",
    "    #Input shape\n",
    "    input_dim = Input(shape=(30,))\n",
    "    #Encoding Layer\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_dim)\n",
    "    #Decoding Layer\n",
    "    decoded = Dense(1, activation='sigmoid')(encoded)\n",
    "\n",
    "    #Model AE\n",
    "    autoencoder = Model(input_dim, decoded)\n",
    "    #Model Encoder \n",
    "    encoder = Model(input_dim, encoded)\n",
    "    #Encoding\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    #Decoding \n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    #Model Decoder \n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "    #optimizer=tf.keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=None, decay=0.0)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model = autoencoder\n",
    "\n",
    "    # Compile model\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "dfInterno=pd.DataFrame()\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [100, 200]\n",
    "epochs = [10]\n",
    "learning_rate = [ 0.1, 0.2]\n",
    "\n",
    "p_grid = dict(batch_size=batch_size, epochs=epochs, learning_rate=learning_rate)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=3, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cb0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\AutoencoderGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b81dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 0.0032 - accuracy: 0.9988 - val_loss: 3.2495e-04 - val_accuracy: 0.9999\n",
      "Epoch 2/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 1.1625e-04 - accuracy: 1.0000 - val_loss: 2.8531e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 9.5921e-06 - accuracy: 1.0000 - val_loss: 3.2740e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 1.5964e-04 - accuracy: 1.0000 - val_loss: 3.1276e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 2.8715e-05 - accuracy: 1.0000 - val_loss: 4.5630e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 3.9381e-05 - accuracy: 1.0000 - val_loss: 4.1982e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 1.0808e-07 - accuracy: 1.0000 - val_loss: 4.4131e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 4.2787e-08 - accuracy: 1.0000 - val_loss: 4.5353e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 2.7944e-08 - accuracy: 1.0000 - val_loss: 4.6952e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 1.6710e-08 - accuracy: 1.0000 - val_loss: 4.8407e-04 - val_accuracy: 1.0000\n",
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45521\n",
      "           1       1.00      1.00      1.00    204911\n",
      "\n",
      "    accuracy                           1.00    250432\n",
      "   macro avg       1.00      1.00      1.00    250432\n",
      "weighted avg       1.00      1.00      1.00    250432\n",
      "\n",
      "Accuracy Score :  1.0\n",
      "Precision Score :  1.0\n",
      "Recall Score :  1.0\n",
      "F1 Score :  1.0\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15174\n",
      "           1       1.00      1.00      1.00     68304\n",
      "\n",
      "    accuracy                           1.00     83478\n",
      "   macro avg       1.00      1.00      1.00     83478\n",
      "weighted avg       1.00      1.00      1.00     83478\n",
      "\n",
      "Accuracy Score :  0.9999760415917966\n",
      "Precision Score :  0.9999760415917966\n",
      "Recall Score :  0.9999760415917966\n",
      "f1 Score :  0.9999760415917966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "scalerS=StandardScaler()\n",
    "scalerM=MinMaxScaler()\n",
    "X_train=MinMaxScaler().fit_transform(X_train)\n",
    "X_train=MinMaxScaler().fit_transform(X_train)\n",
    "\n",
    "X_test=MinMaxScaler().fit_transform(X_test)\n",
    "X_test=MinMaxScaler().fit_transform(X_test)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=100, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"F1 Score : \",f1_score(y_train,y_pred,average='micro'))\n",
    "#################################################################\n",
    "y_pred = model.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"f1 Score : \",f1_score(y_test,y_pred,average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "846726de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'build_fn': <function __main__.create_model(learning_rate=0.01)>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733076a2",
   "metadata": {},
   "source": [
    "# TensorBoard tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39c672d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units': 16, 'dropout': 0.1, 'optimizer': 'adam'}\n",
      "8348/8348 [==============================] - 10s 1ms/step - loss: 0.0501 - accuracy: 0.9900\n",
      "2087/2087 [==============================] - 2s 703us/step - loss: 0.0019 - accuracy: 0.9994\n",
      "--- Starting trial: run-1\n",
      "{'num_units': 16, 'dropout': 0.1, 'optimizer': 'sgd'}\n",
      "8348/8348 [==============================] - 9s 1ms/step - loss: 0.0617 - accuracy: 0.9872\n",
      "2087/2087 [==============================] - 2s 732us/step - loss: 0.0092 - accuracy: 0.9977\n",
      "--- Starting trial: run-2\n",
      "{'num_units': 16, 'dropout': 0.2, 'optimizer': 'adam'}\n",
      "8348/8348 [==============================] - 9s 1ms/step - loss: 0.0346 - accuracy: 0.9936\n",
      "2087/2087 [==============================] - 2s 704us/step - loss: 0.0018 - accuracy: 0.9996\n",
      "--- Starting trial: run-3\n",
      "{'num_units': 16, 'dropout': 0.2, 'optimizer': 'sgd'}\n",
      "8348/8348 [==============================] - 9s 1ms/step - loss: 0.0762 - accuracy: 0.9881\n",
      "2087/2087 [==============================] - 2s 701us/step - loss: 0.0100 - accuracy: 0.9979\n",
      "--- Starting trial: run-4\n",
      "{'num_units': 32, 'dropout': 0.1, 'optimizer': 'adam'}\n",
      "8348/8348 [==============================] - 10s 1ms/step - loss: 0.0264 - accuracy: 0.9951\n",
      "2087/2087 [==============================] - 2s 702us/step - loss: 8.0466e-04 - accuracy: 0.9999\n",
      "--- Starting trial: run-5\n",
      "{'num_units': 32, 'dropout': 0.1, 'optimizer': 'sgd'}\n",
      "8348/8348 [==============================] - 9s 1ms/step - loss: 0.0366 - accuracy: 0.9933\n",
      "2087/2087 [==============================] - 2s 707us/step - loss: 0.0058 - accuracy: 0.9980\n",
      "--- Starting trial: run-6\n",
      "{'num_units': 32, 'dropout': 0.2, 'optimizer': 'adam'}\n",
      "8348/8348 [==============================] - 10s 1ms/step - loss: 0.0319 - accuracy: 0.9933\n",
      "2087/2087 [==============================] - 2s 697us/step - loss: 0.0019 - accuracy: 0.9997\n",
      "--- Starting trial: run-7\n",
      "{'num_units': 32, 'dropout': 0.2, 'optimizer': 'sgd'}\n",
      "8348/8348 [==============================] - 9s 1ms/step - loss: 0.0504 - accuracy: 0.9899\n",
      "2087/2087 [==============================] - 2s 706us/step - loss: 0.0084 - accuracy: 0.9981\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "def train_test_model(hparams):\n",
    "  model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "  ])\n",
    "  model.compile(\n",
    "      optimizer=hparams[HP_OPTIMIZER],\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['accuracy'],\n",
    "  )\n",
    "\n",
    "  model.fit(X_train, y_train, epochs=1) # Run with 1 epoch to speed things up for demo purposes\n",
    "  _, accuracy = model.evaluate(X_test, y_test)\n",
    "  return accuracy\n",
    "\n",
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "      hparams = {\n",
    "          HP_NUM_UNITS: num_units,\n",
    "          HP_DROPOUT: dropout_rate,\n",
    "          HP_OPTIMIZER: optimizer,\n",
    "      }\n",
    "      run_name = \"run-%d\" % session_num\n",
    "      print('--- Starting trial: %s' % run_name)\n",
    "      print({h.name: hparams[h] for h in hparams})\n",
    "      run('logs/hparam_tuning/' + run_name, hparams)\n",
    "      session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dd29866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b7a270c9c62eb39\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b7a270c9c62eb39\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36231934",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5298e31",
   "metadata": {},
   "source": [
    "# Part 2 - Building the RNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LSTM, Dropout, Input\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "hidden_nodes = int(2/3 * (X_train.shape[1] + 1))\n",
    "#print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the RNN\n",
    "def create_model():\n",
    "    #Initializing the RNN\n",
    "    model = Sequential()\n",
    "    #Adding the first LSTM layer and dropout \n",
    "    model.add(LSTM(units=21, return_sequences=False, input_shape=(30, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    #Adding a second LSTM Layer and dropout\n",
    "    #model.add(LSTM(units=20, return_sequences=False))\n",
    "    #model.add(Dropout(0.2))\n",
    "    #Adding the output layer\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "    #Output is two classes with only one present at a time. \n",
    "    #Softmax allows the model to interpret the outputs as probabilities\n",
    "\n",
    "    #Compiling the model without training\n",
    "    #Loss function is chosen together with activation. Softmax points us to binary crossentropy\n",
    "    #since we are faced with a binary classification problem\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    opt = SGD(lr=0.01)\n",
    "\n",
    "    model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0ed91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "268/268 [==============================] - 11s 35ms/step - loss: 0.2955 - acc: 0.8650\n",
      "Epoch 2/20\n",
      "268/268 [==============================] - 9s 34ms/step - loss: 0.1444 - acc: 0.9482\n",
      "Epoch 3/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.1083 - acc: 0.9638\n",
      "Epoch 4/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0934 - acc: 0.9733\n",
      "Epoch 5/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0811 - acc: 0.9786\n",
      "Epoch 6/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0731 - acc: 0.9816\n",
      "Epoch 7/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0690 - acc: 0.9827\n",
      "Epoch 8/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0601 - acc: 0.9847\n",
      "Epoch 9/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0514 - acc: 0.9864\n",
      "Epoch 10/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0395 - acc: 0.9892\n",
      "Epoch 11/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0322 - acc: 0.9907\n",
      "Epoch 12/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0434 - acc: 0.9866\n",
      "Epoch 13/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0247 - acc: 0.9925\n",
      "Epoch 14/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0213 - acc: 0.9936\n",
      "Epoch 15/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0412 - acc: 0.9874\n",
      "Epoch 16/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0223 - acc: 0.9930\n",
      "Epoch 17/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0184 - acc: 0.9947\n",
      "Epoch 18/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0168 - acc: 0.9954\n",
      "Epoch 19/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0160 - acc: 0.9956\n",
      "Epoch 20/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0137 - acc: 0.9965\n",
      "{'batch_size': 1000, 'epochs': 20}\n",
      "0.9978549593198338\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "268/268 [==============================] - 10s 33ms/step - loss: 0.2756 - acc: 0.8880\n",
      "Epoch 2/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.1325 - acc: 0.9470\n",
      "Epoch 3/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0901 - acc: 0.9712\n",
      "Epoch 4/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0661 - acc: 0.9815\n",
      "Epoch 5/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0564 - acc: 0.9843\n",
      "Epoch 6/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0482 - acc: 0.9868: 0s - loss: 0.048\n",
      "Epoch 7/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0422 - acc: 0.9887\n",
      "Epoch 8/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0357 - acc: 0.9906\n",
      "Epoch 9/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0286 - acc: 0.9926: 1s - loss: 0.0299 - acc: 0.992 - ETA: 1s -\n",
      "Epoch 10/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0202 - acc: 0.9950\n",
      "Epoch 11/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0142 - acc: 0.9971\n",
      "Epoch 12/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0119 - acc: 0.9977\n",
      "Epoch 13/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0104 - acc: 0.9980\n",
      "Epoch 14/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0095 - acc: 0.9981\n",
      "Epoch 15/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0084 - acc: 0.9983\n",
      "Epoch 16/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0078 - acc: 0.9984\n",
      "Epoch 17/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0070 - acc: 0.9985\n",
      "Epoch 18/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0068 - acc: 0.9987\n",
      "Epoch 19/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0059 - acc: 0.9988\n",
      "Epoch 20/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0057 - acc: 0.9989\n",
      "{'batch_size': 1000, 'epochs': 20}\n",
      "0.9983977696263271\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "268/268 [==============================] - 11s 33ms/step - loss: 0.2894 - acc: 0.8778 \n",
      "Epoch 2/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.1487 - acc: 0.9409\n",
      "Epoch 3/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0870 - acc: 0.9706\n",
      "Epoch 4/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0667 - acc: 0.9773\n",
      "Epoch 5/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0558 - acc: 0.9820\n",
      "Epoch 6/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0493 - acc: 0.9845\n",
      "Epoch 7/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0417 - acc: 0.9866\n",
      "Epoch 8/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0367 - acc: 0.9880\n",
      "Epoch 9/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0305 - acc: 0.9902\n",
      "Epoch 10/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0261 - acc: 0.9919\n",
      "Epoch 11/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0221 - acc: 0.9934\n",
      "Epoch 12/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0185 - acc: 0.9952: 0s - loss: 0.0186 - \n",
      "Epoch 13/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0156 - acc: 0.9966: \n",
      "Epoch 14/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0140 - acc: 0.9972\n",
      "Epoch 15/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0116 - acc: 0.9976: 3s -  - ETA: 1s -\n",
      "Epoch 16/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0110 - acc: 0.9978\n",
      "Epoch 17/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0107 - acc: 0.9979\n",
      "Epoch 18/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0096 - acc: 0.9981\n",
      "Epoch 19/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0100 - acc: 0.9980\n",
      "Epoch 20/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0085 - acc: 0.9984\n",
      "{'batch_size': 1000, 'epochs': 20}\n",
      "0.9953318299907817\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "268/268 [==============================] - 10s 33ms/step - loss: 0.2734 - acc: 0.8954\n",
      "Epoch 2/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.1332 - acc: 0.9469: 2s - loss: 0.1417 - ac - ETA\n",
      "Epoch 3/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0906 - acc: 0.9708\n",
      "Epoch 4/20\n",
      "268/268 [==============================] - 9s 34ms/step - loss: 0.0652 - acc: 0.9805\n",
      "Epoch 5/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0539 - acc: 0.9841\n",
      "Epoch 6/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0418 - acc: 0.9877\n",
      "Epoch 7/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0245 - acc: 0.9935\n",
      "Epoch 8/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0172 - acc: 0.9963\n",
      "Epoch 9/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0139 - acc: 0.9974: 0s - loss: 0.0139 - a\n",
      "Epoch 10/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0123 - acc: 0.9978\n",
      "Epoch 11/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0111 - acc: 0.9982\n",
      "Epoch 12/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0103 - acc: 0.9984\n",
      "Epoch 13/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0098 - acc: 0.9984\n",
      "Epoch 14/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0096 - acc: 0.9984: 0s - loss: 0.0096 - acc: 0. - ETA: 0s - loss: 0.0095 - acc: 0.\n",
      "Epoch 15/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0089 - acc: 0.9985\n",
      "Epoch 16/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0086 - acc: 0.9985: 0s - loss: 0.0085 - \n",
      "Epoch 17/20\n",
      "268/268 [==============================] - 9s 34ms/step - loss: 0.0080 - acc: 0.9987: 0s - loss: 0.008\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0076 - acc: 0.9987\n",
      "Epoch 19/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0072 - acc: 0.9988\n",
      "Epoch 20/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0411 - acc: 0.9939\n",
      "{'batch_size': 1000, 'epochs': 20}\n",
      "0.9981544400950565\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "268/268 [==============================] - 10s 33ms/step - loss: 0.2747 - acc: 0.8950\n",
      "Epoch 2/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.1444 - acc: 0.9489\n",
      "Epoch 3/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.1006 - acc: 0.9647\n",
      "Epoch 4/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0625 - acc: 0.9831\n",
      "Epoch 5/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0512 - acc: 0.9857\n",
      "Epoch 6/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0453 - acc: 0.9867\n",
      "Epoch 7/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0396 - acc: 0.9885: \n",
      "Epoch 8/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0334 - acc: 0.9910\n",
      "Epoch 9/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0241 - acc: 0.9930\n",
      "Epoch 10/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0192 - acc: 0.9955\n",
      "Epoch 11/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0154 - acc: 0.9968\n",
      "Epoch 12/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0132 - acc: 0.9975\n",
      "Epoch 13/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0132 - acc: 0.9974\n",
      "Epoch 14/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0115 - acc: 0.9977: 0s - loss: 0.0118 - acc\n",
      "Epoch 15/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0103 - acc: 0.9979\n",
      "Epoch 16/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0099 - acc: 0.9980\n",
      "Epoch 17/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0093 - acc: 0.9980\n",
      "Epoch 18/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0088 - acc: 0.9982\n",
      "Epoch 19/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0078 - acc: 0.9983\n",
      "Epoch 20/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0077 - acc: 0.9983\n",
      "{'batch_size': 1000, 'epochs': 20}\n",
      "0.9981432088372564\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LSTM, Dropout, Input\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "dfInterno=pd.DataFrame()\n",
    "\n",
    "hidden_nodes = int(2/3 * (X.shape[1] + 1))\n",
    "#print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the RNN\n",
    "def create_model():\n",
    "    #Initializing the RNN\n",
    "    model = Sequential()\n",
    "    #Adding the first LSTM layer and dropout \n",
    "    model.add(LSTM(units=21, return_sequences=False, input_shape=(30, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    #Adding the output layer\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "    #Output is two classes with only one present at a time. \n",
    "    #Softmax allows the model to interpret the outputs as probabilities\n",
    "\n",
    "    #Compiling the model without training\n",
    "    #Loss function is chosen together with activation. Softmax points us to binary crossentropy\n",
    "    #since we are faced with a binary classification problem\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    opt = SGD(lr=0.01)\n",
    "\n",
    "    model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [1000,5000]\n",
    "epochs = [10, 20]\n",
    "\n",
    "p_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=-1, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    \n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0d00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\RNNGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1484a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "251/251 [==============================] - 12s 40ms/step - loss: 0.4803 - acc: 0.8068 - val_loss: 0.2789 - val_acc: 0.8846\n",
      "Epoch 2/20\n",
      "251/251 [==============================] - 10s 38ms/step - loss: 0.2200 - acc: 0.9337 - val_loss: 0.1956 - val_acc: 0.9448\n",
      "Epoch 3/20\n",
      "251/251 [==============================] - 10s 38ms/step - loss: 0.1985 - acc: 0.9441 - val_loss: 0.1902 - val_acc: 0.9428\n",
      "Epoch 4/20\n",
      "251/251 [==============================] - 10s 38ms/step - loss: 0.1887 - acc: 0.9453 - val_loss: 0.1790 - val_acc: 0.9471\n",
      "Epoch 5/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.1751 - acc: 0.9482 - val_loss: 0.1650 - val_acc: 0.9504\n",
      "Epoch 6/20\n",
      "251/251 [==============================] - 10s 38ms/step - loss: 0.1681 - acc: 0.9488 - val_loss: 0.1565 - val_acc: 0.9505\n",
      "Epoch 7/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1618 - acc: 0.9488 - val_loss: 0.1461 - val_acc: 0.9518\n",
      "Epoch 8/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1481 - acc: 0.9507 - val_loss: 0.1305 - val_acc: 0.9549\n",
      "Epoch 9/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1323 - acc: 0.9526 - val_loss: 0.1156 - val_acc: 0.9595\n",
      "Epoch 10/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1178 - acc: 0.9565 - val_loss: 0.1076 - val_acc: 0.9624\n",
      "Epoch 11/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1110 - acc: 0.9595 - val_loss: 0.0971 - val_acc: 0.9628\n",
      "Epoch 12/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1013 - acc: 0.9629 - val_loss: 0.0904 - val_acc: 0.9644\n",
      "Epoch 13/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.0935 - acc: 0.9668 - val_loss: 0.0851 - val_acc: 0.9683\n",
      "Epoch 14/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1379 - acc: 0.9575 - val_loss: 0.2229 - val_acc: 0.9448\n",
      "Epoch 15/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1717 - acc: 0.9511 - val_loss: 0.1045 - val_acc: 0.9672\n",
      "Epoch 16/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1095 - acc: 0.9689 - val_loss: 0.0863 - val_acc: 0.9771\n",
      "Epoch 17/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.0919 - acc: 0.9770 - val_loss: 0.0771 - val_acc: 0.9796\n",
      "Epoch 18/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.0873 - acc: 0.9777 - val_loss: 0.0758 - val_acc: 0.9791\n",
      "Epoch 19/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1926 - acc: 0.9313 - val_loss: 0.1343 - val_acc: 0.9541\n",
      "Epoch 20/20\n",
      "251/251 [==============================] - 9s 37ms/step - loss: 0.1234 - acc: 0.9589 - val_loss: 0.0873 - val_acc: 0.9777\n",
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93     45521\n",
      "           1       0.98      1.00      0.99    204911\n",
      "\n",
      "    accuracy                           0.98    250432\n",
      "   macro avg       0.98      0.94      0.96    250432\n",
      "weighted avg       0.98      0.98      0.98    250432\n",
      "\n",
      "Accuracy Score :  0.9773191924354715\n",
      "Precision Score :  0.9773191924354715\n",
      "Recall Score :  0.9773191924354715\n",
      "F1 Score :  0.9773191924354715\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.94     15174\n",
      "           1       0.98      1.00      0.99     68304\n",
      "\n",
      "    accuracy                           0.98     83478\n",
      "   macro avg       0.98      0.94      0.96     83478\n",
      "weighted avg       0.98      0.98      0.98     83478\n",
      "\n",
      "Accuracy Score :  0.9777426387790795\n",
      "Precision Score :  0.9777426387790795\n",
      "Recall Score :  0.9777426387790795\n",
      "f1 Score :  0.9777426387790795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "scalerS=StandardScaler()\n",
    "scalerM=MinMaxScaler()\n",
    "X_train=MinMaxScaler().fit_transform(X_train)\n",
    "X_train=MinMaxScaler().fit_transform(X_train)\n",
    "\n",
    "X_test=MinMaxScaler().fit_transform(X_test)\n",
    "X_test=MinMaxScaler().fit_transform(X_test)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=20, validation_data=(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"F1 Score : \",f1_score(y_train,y_pred,average='micro'))\n",
    "#################################################################\n",
    "y_pred = model.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"f1 Score : \",f1_score(y_test,y_pred,average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754c114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "907d122e",
   "metadata": {},
   "source": [
    "# Trying it out on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdca6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfO.copy()\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "# Part 2 - Building the RNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "\n",
    "hidden_nodes = int(2/3 * (X_train.shape[1] + 1))\n",
    "#print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the RNN\n",
    "def create_model():\n",
    "#Enc Dimension \n",
    "    encoding_dim=100\n",
    "    #Input shape\n",
    "    input_dim = Input(shape=(61,))\n",
    "    #Encoding Layer\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_dim)\n",
    "    #Decoding Layer\n",
    "    decoded = Dense(1, activation='sigmoid')(encoded)\n",
    "\n",
    "    #Model AE\n",
    "    autoencoder = Model(input_dim, decoded)\n",
    "    #Model Encoder \n",
    "    encoder = Model(input_dim, encoded)\n",
    "    #Encoding\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    #Decoding \n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    #Model Decoder \n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "    #optimizer=tf.keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=None, decay=0.0)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model = autoencoder\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde10338",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_).to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\WholeGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38467c5",
   "metadata": {},
   "source": [
    "# Building the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded87288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the CNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "X = df.copy()\n",
    "#X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "#Adding the extra dimension CNN needs\n",
    "SX_train=StandardScaler().fit_transform(X_train)\n",
    "\n",
    "X_train=X_train.to_numpy()\n",
    "X_train = SX_train[..., np.newaxis]\n",
    "X_test=X_test.to_numpy()\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the CNN\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=1, kernel_size=3, activation='relu'))\n",
    "    #model.add(Conv1D(filters=31, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=\"sigmoid\")) # binary activation output\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "#print(results_nm)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(f1_score(y_test,y_pred,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the CNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the CNN\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=31, kernel_size=3, activation='relu'))\n",
    "    #model.add(Conv1D(filters=31, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=\"sigmoid\")) # binary activation output\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "#model.fit(X_train, y_train, batch_size=1000, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [32, 64]\n",
    "#optimizer= [\"RMSprop\", \"adam\"]\n",
    "epochs = [10, 20]\n",
    "#dropout = [0, 0.1, 0.2]\n",
    "p_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "    X_train=X_train.to_numpy()\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_test=X_test.to_numpy()\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "    #y_train= y_train.to_numpy()\n",
    "    #y_test= y_test.to_numpy()\n",
    "\n",
    "        \n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=-1, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_).to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\CNNGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3559e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f8b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
