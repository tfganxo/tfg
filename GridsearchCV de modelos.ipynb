{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28182ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "import mglearn\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Random global\n",
    "seed = 12345\n",
    "rng = np.random.default_rng(seed)  # can be called without a seed\n",
    "rng.random()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv (r\"C:\\Users\\White\\Desktop\\tfg\\generados\\RFECved.csv\")\n",
    "dfNon = pd.read_csv (r\"C:\\Users\\White\\Desktop\\tfg\\generados\\scaled.csv\")\n",
    "df2 = pd.read_csv (r\"C:\\Users\\White\\Desktop\\tfg\\generados\\Deoutliers2.csv\")\n",
    "dfR = pd.read_csv (r\"C:\\Users\\White\\Desktop\\tfg\\generados\\Raw-ishDf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5541837",
   "metadata": {},
   "source": [
    "# GaussianNB as a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d904c8",
   "metadata": {},
   "source": [
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee837515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'var_smoothing': 1e-08}\n",
      "0.9864184815803458\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'var_smoothing': 1e-08}\n",
      "0.986673067244306\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'var_smoothing': 1e-08}\n",
      "0.9863698248252482\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'var_smoothing': 1e-05}\n",
      "0.9841536627722809\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "{'var_smoothing': 1e-08}\n",
      "0.9847713463556648\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "dfInterno=pd.DataFrame()\n",
    "dfBest=pd.DataFrame()\n",
    "\n",
    "#baseline parameters\n",
    "p_grid = { \n",
    "    'var_smoothing': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "}\n",
    "\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    grid = GridSearchCV(GaussianNB(),return_train_score=True,scoring=scoring, refit='accuracy', param_grid=p_grid, n_jobs=-1, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=3)\n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21d0ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\GaussianGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08788c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     45521\n",
      "           1       1.00      0.99      0.99    204911\n",
      "\n",
      "    accuracy                           0.99    250432\n",
      "   macro avg       0.97      0.99      0.98    250432\n",
      "weighted avg       0.99      0.99      0.99    250432\n",
      "\n",
      "Accuracy Score :  0.9881045553283926\n",
      "Precision Score :  0.9987206717708076\n",
      "Recall Score :  0.986725944434413\n",
      "F1 Score :  0.9926870760823937\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     15174\n",
      "           1       1.00      0.99      0.99     68304\n",
      "\n",
      "    accuracy                           0.99     83478\n",
      "   macro avg       0.97      0.99      0.98     83478\n",
      "weighted avg       0.99      0.99      0.99     83478\n",
      "\n",
      "Accuracy Score :  0.9879010038573037\n",
      "Precision Score :  0.9987992172675522\n",
      "Recall Score :  0.9863990395877255\n",
      "F1 Score :  0.9925604007071303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Results tested on the whole dataset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "##############################################################\n",
    "gnb = GaussianNB(var_smoothing=1e-08)\n",
    "pipe= make_pipeline(MinMaxScaler(),StandardScaler(),gnb)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \",f1_score(y_train,y_pred,average='binary'))\n",
    "#################################################################\n",
    "y_pred = pipe.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score : \", accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \", f1_score(y_test,y_pred,average='binary'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b692e6",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "315bd770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'max_depth': 5, 'n_estimators': 200}\n",
      "0.999363601040338\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'max_depth': 5, 'n_estimators': 200}\n",
      "0.9994085225680325\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'max_depth': 5, 'n_estimators': 100}\n",
      "0.9994010361372409\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'max_depth': 5, 'n_estimators': 200}\n",
      "0.9994197534754818\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'max_depth': 5, 'n_estimators': 200}\n",
      "0.9993823190792801\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "dfInterno=pd.DataFrame()\n",
    "# define the model with default hyperparameters\n",
    "model=RandomForestClassifier(random_state=42)\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "# define the grid of values to search\n",
    "p_grid = { \n",
    "    'n_estimators': [100, 200],\n",
    "    #Empirical good default values [...] max_features=sqrt(n_features) for classification tasks\n",
    "   # 'max_features': [\"auto\", \"sqrt\"],\n",
    "    'max_depth' : [3,4,5],\n",
    "    #'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=-1, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0caeff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\RForestGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1c89881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45521\n",
      "           1       1.00      1.00      1.00    204911\n",
      "\n",
      "    accuracy                           1.00    250432\n",
      "   macro avg       1.00      1.00      1.00    250432\n",
      "weighted avg       1.00      1.00      1.00    250432\n",
      "\n",
      "Accuracy Score :  0.9994090212113468\n",
      "Precision Score :  0.9992782565017873\n",
      "Recall Score :  1.0\n",
      "F1 Score :  0.9996389979754616\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15174\n",
      "           1       1.00      1.00      1.00     68304\n",
      "\n",
      "    accuracy                           1.00     83478\n",
      "   macro avg       1.00      1.00      1.00     83478\n",
      "weighted avg       1.00      1.00      1.00     83478\n",
      "\n",
      "Accuracy Score :  0.999329164570306\n",
      "Precision Score :  0.9992246247476373\n",
      "Recall Score :  0.9999560787069571\n",
      "F1 Score :  0.999590217916258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Results tested on the whole dataset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "rfc=RandomForestClassifier(max_depth= 5, max_features='sqrt', n_estimators= 100, random_state=42)\n",
    "pipe= make_pipeline(MinMaxScaler(),StandardScaler(),rfc)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \",f1_score(y_train,y_pred,average='binary'))\n",
    "#################################################################\n",
    "y_pred = pipe.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score : \", accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \", f1_score(y_test,y_pred,average='binary'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60925434",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b040d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.9988395072312443\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.9987758670339766\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.9988170456966257\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.9987983276576836\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "0.9988170470279579\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "dfInterno=pd.DataFrame()\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# define the model with default hyperparameters\n",
    "model = AdaBoostClassifier()\n",
    "# define the grid of values to search\n",
    "p_grid = dict()\n",
    "p_grid['n_estimators'] = [100,200]\n",
    "p_grid['learning_rate'] = [0.1,0.001,0.0001]\n",
    "\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=-1, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17231836",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\AdaboostGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fe78832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45521\n",
      "           1       1.00      1.00      1.00    204911\n",
      "\n",
      "    accuracy                           1.00    250432\n",
      "   macro avg       1.00      1.00      1.00    250432\n",
      "weighted avg       1.00      1.00      1.00    250432\n",
      "\n",
      "Accuracy Score :  0.9988180424226936\n",
      "Precision Score :  0.9990341981649765\n",
      "Recall Score :  0.9995217435862399\n",
      "F1 Score :  0.9992779114075361\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15174\n",
      "           1       1.00      1.00      1.00     68304\n",
      "\n",
      "    accuracy                           1.00     83478\n",
      "   macro avg       1.00      1.00      1.00     83478\n",
      "weighted avg       1.00      1.00      1.00     83478\n",
      "\n",
      "Accuracy Score :  0.9987421835693236\n",
      "Precision Score :  0.9989318896773721\n",
      "Recall Score :  0.9995315062075427\n",
      "F1 Score :  0.9992316079883496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "adaB = AdaBoostClassifier(learning_rate= 0.1, n_estimators= 200)\n",
    "\n",
    "pipe= make_pipeline(MinMaxScaler(),StandardScaler(),adaB)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \",f1_score(y_train,y_pred,average='binary'))\n",
    "#################################################################\n",
    "y_pred = pipe.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score : \", accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \", f1_score(y_test,y_pred,average='binary'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9266a08",
   "metadata": {},
   "source": [
    "# LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70877f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'C': 1, 'tol': 0.0001}\n",
      "0.9988432513575514\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'C': 1, 'tol': 0.001}\n",
      "0.9989592997935859\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'C': 1, 'tol': 0.0001}\n",
      "0.9989705304908248\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'C': 1, 'tol': 0.0001}\n",
      "0.998951812101532\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "{'C': 1, 'tol': 0.0001}\n",
      "0.9988806872252256\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "dfInterno=pd.DataFrame()\n",
    "\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = LinearSVC()\n",
    "# define the grid of values to search\n",
    "p_grid = {\"tol\": [1e-3, 1e-4], \"C\": [1, 10, 100]}\n",
    "\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=2, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "811bb930",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\LinearSVCGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dfa68fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'loss', 'max_iter', 'multi_class', 'penalty', 'random_state', 'tol', 'verbose'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6a80c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45521\n",
      "           1       1.00      1.00      1.00    204911\n",
      "\n",
      "    accuracy                           1.00    250432\n",
      "   macro avg       1.00      1.00      1.00    250432\n",
      "weighted avg       1.00      1.00      1.00    250432\n",
      "\n",
      "Accuracy Score :  0.9989617940199336\n",
      "Precision Score :  0.9991268676009346\n",
      "Recall Score :  0.9996047064335248\n",
      "F1 Score :  0.9993657298985168\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15174\n",
      "           1       1.00      1.00      1.00     68304\n",
      "\n",
      "    accuracy                           1.00     83478\n",
      "   macro avg       1.00      1.00      1.00     83478\n",
      "weighted avg       1.00      1.00      1.00     83478\n",
      "\n",
      "Accuracy Score :  0.9988979132226455\n",
      "Precision Score :  0.9990781117387105\n",
      "Recall Score :  0.9995754275005856\n",
      "F1 Score :  0.9993267077472519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "LSVC = LinearSVC(C= 1, tol= 0.0001)\n",
    "\n",
    "pipe= make_pipeline(MinMaxScaler(),StandardScaler(),LSVC)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \",f1_score(y_train,y_pred,average='binary'))\n",
    "#################################################################\n",
    "y_pred = pipe.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score : \", accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \", f1_score(y_test,y_pred,average='binary'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8279a7",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcc46bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/10\n",
      "2672/2672 [==============================] - 3s 934us/step - loss: 0.6063 - accuracy: 0.7653\n",
      "Epoch 2/10\n",
      "2672/2672 [==============================] - 3s 937us/step - loss: 0.3872 - accuracy: 0.9617\n",
      "Epoch 3/10\n",
      "2672/2672 [==============================] - 2s 928us/step - loss: 0.2442 - accuracy: 0.9812\n",
      "Epoch 4/10\n",
      "2672/2672 [==============================] - 2s 927us/step - loss: 0.1539 - accuracy: 0.9875\n",
      "Epoch 5/10\n",
      "2672/2672 [==============================] - 2s 931us/step - loss: 0.0980 - accuracy: 0.9896\n",
      "Epoch 6/10\n",
      "2672/2672 [==============================] - 2s 932us/step - loss: 0.0640 - accuracy: 0.9927\n",
      "Epoch 7/10\n",
      "2672/2672 [==============================] - 3s 937us/step - loss: 0.0433 - accuracy: 0.9945\n",
      "Epoch 8/10\n",
      "2672/2672 [==============================] - 2s 932us/step - loss: 0.0307 - accuracy: 0.9972\n",
      "Epoch 9/10\n",
      "2672/2672 [==============================] - 2s 926us/step - loss: 0.0230 - accuracy: 0.9976\n",
      "Epoch 10/10\n",
      "2672/2672 [==============================] - 2s 927us/step - loss: 0.0181 - accuracy: 0.9976\n",
      "{'batch_size': 100, 'epochs': 10}\n",
      "0.9977201938258388\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/10\n",
      "2672/2672 [==============================] - 3s 934us/step - loss: 0.4872 - accuracy: 0.8711\n",
      "Epoch 2/10\n",
      "2672/2672 [==============================] - 3s 936us/step - loss: 0.3054 - accuracy: 0.9607\n",
      "Epoch 3/10\n",
      "2672/2672 [==============================] - 2s 931us/step - loss: 0.1949 - accuracy: 0.9717\n",
      "Epoch 4/10\n",
      "2672/2672 [==============================] - 2s 924us/step - loss: 0.1265 - accuracy: 0.9822\n",
      "Epoch 5/10\n",
      "2672/2672 [==============================] - 2s 928us/step - loss: 0.0842 - accuracy: 0.9864\n",
      "Epoch 6/10\n",
      "2672/2672 [==============================] - 2s 919us/step - loss: 0.0580 - accuracy: 0.9898\n",
      "Epoch 7/10\n",
      "2672/2672 [==============================] - 3s 938us/step - loss: 0.0413 - accuracy: 0.9939\n",
      "Epoch 8/10\n",
      "2672/2672 [==============================] - 2s 924us/step - loss: 0.0304 - accuracy: 0.9966\n",
      "Epoch 9/10\n",
      "2672/2672 [==============================] - 2s 925us/step - loss: 0.0232 - accuracy: 0.9969\n",
      "Epoch 10/10\n",
      "2672/2672 [==============================] - 2s 928us/step - loss: 0.0184 - accuracy: 0.9971\n",
      "{'batch_size': 100, 'epochs': 10}\n",
      "0.9973233807616309\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/10\n",
      "2672/2672 [==============================] - 3s 939us/step - loss: 0.7030 - accuracy: 0.4915\n",
      "Epoch 2/10\n",
      "2672/2672 [==============================] - 2s 928us/step - loss: 0.4498 - accuracy: 0.9484\n",
      "Epoch 3/10\n",
      "2672/2672 [==============================] - 2s 928us/step - loss: 0.2911 - accuracy: 0.9865\n",
      "Epoch 4/10\n",
      "2672/2672 [==============================] - 2s 927us/step - loss: 0.1887 - accuracy: 0.9916\n",
      "Epoch 5/10\n",
      "2672/2672 [==============================] - 3s 938us/step - loss: 0.1222 - accuracy: 0.9951\n",
      "Epoch 6/10\n",
      "2672/2672 [==============================] - 3s 936us/step - loss: 0.0794 - accuracy: 0.9968\n",
      "Epoch 7/10\n",
      "2672/2672 [==============================] - 3s 936us/step - loss: 0.0524 - accuracy: 0.9972\n",
      "Epoch 8/10\n",
      "2672/2672 [==============================] - 3s 939us/step - loss: 0.0357 - accuracy: 0.9973\n",
      "Epoch 9/10\n",
      "2672/2672 [==============================] - 2s 935us/step - loss: 0.0255 - accuracy: 0.9975\n",
      "Epoch 10/10\n",
      "2672/2672 [==============================] - 3s 954us/step - loss: 0.0193 - accuracy: 0.9976\n",
      "{'batch_size': 100, 'epochs': 10}\n",
      "0.9977351693500862\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/10\n",
      "2672/2672 [==============================] - 3s 953us/step - loss: 0.5305 - accuracy: 0.8714\n",
      "Epoch 2/10\n",
      "2672/2672 [==============================] - 3s 940us/step - loss: 0.3228 - accuracy: 0.9878\n",
      "Epoch 3/10\n",
      "2672/2672 [==============================] - 3s 949us/step - loss: 0.2011 - accuracy: 0.9924\n",
      "Epoch 4/10\n",
      "2672/2672 [==============================] - 3s 938us/step - loss: 0.1272 - accuracy: 0.9947\n",
      "Epoch 5/10\n",
      "2672/2672 [==============================] - 3s 949us/step - loss: 0.0811 - accuracy: 0.9969\n",
      "Epoch 6/10\n",
      "2672/2672 [==============================] - 2s 931us/step - loss: 0.0528 - accuracy: 0.9975\n",
      "Epoch 7/10\n",
      "2672/2672 [==============================] - 2s 934us/step - loss: 0.0359 - accuracy: 0.9976\n",
      "Epoch 8/10\n",
      "2672/2672 [==============================] - 3s 940us/step - loss: 0.0258 - accuracy: 0.9977\n",
      "Epoch 9/10\n",
      "2672/2672 [==============================] - 3s 945us/step - loss: 0.0197 - accuracy: 0.9977\n",
      "Epoch 10/10\n",
      "2672/2672 [==============================] - 2s 927us/step - loss: 0.0160 - accuracy: 0.9978\n",
      "{'batch_size': 100, 'epochs': 10}\n",
      "0.9976078858023959\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/10\n",
      "2672/2672 [==============================] - 3s 953us/step - loss: 0.6141 - accuracy: 0.8175\n",
      "Epoch 2/10\n",
      "2672/2672 [==============================] - 3s 947us/step - loss: 0.3909 - accuracy: 0.9420\n",
      "Epoch 3/10\n",
      "2672/2672 [==============================] - 3s 957us/step - loss: 0.2486 - accuracy: 0.9847\n",
      "Epoch 4/10\n",
      "2672/2672 [==============================] - 3s 952us/step - loss: 0.1582 - accuracy: 0.9940\n",
      "Epoch 5/10\n",
      "2672/2672 [==============================] - 3s 949us/step - loss: 0.1008 - accuracy: 0.9959\n",
      "Epoch 6/10\n",
      "2672/2672 [==============================] - 3s 956us/step - loss: 0.0647 - accuracy: 0.9973\n",
      "Epoch 7/10\n",
      "2672/2672 [==============================] - 2s 927us/step - loss: 0.0424 - accuracy: 0.9975\n",
      "Epoch 8/10\n",
      "2672/2672 [==============================] - 2s 932us/step - loss: 0.0289 - accuracy: 0.9977\n",
      "Epoch 9/10\n",
      "2672/2672 [==============================] - 2s 928us/step - loss: 0.0209 - accuracy: 0.9979\n",
      "Epoch 10/10\n",
      "2672/2672 [==============================] - 2s 922us/step - loss: 0.0161 - accuracy: 0.9979\n",
      "{'batch_size': 100, 'epochs': 10}\n",
      "0.9974993240073635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "#Autoencoder\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learning_rate=0.1):\n",
    "    #Enc Dimension \n",
    "    encoding_dim=100\n",
    "    #Input shape\n",
    "    input_dim = Input(shape=(30,))\n",
    "    #Encoding Layer\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_dim)\n",
    "    #Decoding Layer\n",
    "    decoded = Dense(1, activation='sigmoid')(encoded)\n",
    "\n",
    "    #Model AE\n",
    "    autoencoder = Model(input_dim, decoded)\n",
    "    #Model Encoder \n",
    "    encoder = Model(input_dim, encoded)\n",
    "    #Encoding\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    #Decoding \n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    #Model Decoder \n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "    #optimizer=tf.keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=None, decay=0.0)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model = autoencoder\n",
    "\n",
    "    # Compile model\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "dfInterno=pd.DataFrame()\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [100, 200]\n",
    "epochs = [10]\n",
    "\n",
    "p_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=3, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cb0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\AutoencoderGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b81dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2505/2505 [==============================] - 5s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 4.1167e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 2.8533e-04 - accuracy: 0.9999 - val_loss: 7.2312e-04 - val_accuracy: 0.9998\n",
      "Epoch 3/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 1.0262e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "2505/2505 [==============================] - 3s 1ms/step - loss: 6.2898e-04 - accuracy: 0.9999 - val_loss: 0.0017 - val_accuracy: 0.9999\n",
      "Epoch 5/10\n",
      "2505/2505 [==============================] - 4s 1ms/step - loss: 4.6502e-05 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "2505/2505 [==============================] - 4s 2ms/step - loss: 4.2618e-05 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "2505/2505 [==============================] - 4s 2ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.0097 - val_accuracy: 0.9995\n",
      "Epoch 8/10\n",
      "2505/2505 [==============================] - 4s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "2505/2505 [==============================] - 4s 2ms/step - loss: 9.7800e-06 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "2505/2505 [==============================] - 4s 1ms/step - loss: 2.5603e-06 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45521\n",
      "           1       1.00      1.00      1.00    204911\n",
      "\n",
      "    accuracy                           1.00    250432\n",
      "   macro avg       1.00      1.00      1.00    250432\n",
      "weighted avg       1.00      1.00      1.00    250432\n",
      "\n",
      "Accuracy Score :  1.0\n",
      "Precision Score :  1.0\n",
      "Recall Score :  1.0\n",
      "F1 Score :  1.0\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15174\n",
      "           1       1.00      1.00      1.00     68304\n",
      "\n",
      "    accuracy                           1.00     83478\n",
      "   macro avg       1.00      1.00      1.00     83478\n",
      "weighted avg       1.00      1.00      1.00     83478\n",
      "\n",
      "Accuracy Score :  0.9999760415917966\n",
      "Precision Score :  0.9999853595689857\n",
      "Recall Score :  0.9999853595689857\n",
      "F1 Score :  0.9999853595689857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "def create_model(learning_rate=0.1):\n",
    "    #Enc Dimension \n",
    "    encoding_dim=100\n",
    "    #Input shape\n",
    "    input_dim = Input(shape=(30,))\n",
    "    #Encoding Layer\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_dim)\n",
    "    #Decoding Layer\n",
    "    decoded = Dense(1, activation='sigmoid')(encoded)\n",
    "\n",
    "    #Model AE\n",
    "    autoencoder = Model(input_dim, decoded)\n",
    "    #Model Encoder \n",
    "    encoder = Model(input_dim, encoded)\n",
    "    #Encoding\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    #Decoding \n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    #Model Decoder \n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "    #optimizer=tf.keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=None, decay=0.0)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model = autoencoder\n",
    "    # Compile model\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "scalerS=StandardScaler()\n",
    "scalerM=MinMaxScaler()\n",
    "X_train=MinMaxScaler().fit_transform(X_train)\n",
    "X_train=MinMaxScaler().fit_transform(X_train)\n",
    "\n",
    "X_test=MinMaxScaler().fit_transform(X_test)\n",
    "X_test=MinMaxScaler().fit_transform(X_test)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=100, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \",f1_score(y_train,y_pred,average='binary'))\n",
    "#################################################################\n",
    "y_pred = model.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score : \", accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \", f1_score(y_test,y_pred,average='binary'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "846726de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'build_fn': <function __main__.create_model(learning_rate=0.01)>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733076a2",
   "metadata": {},
   "source": [
    "# TensorBoard tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39c672d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units': 16, 'dropout': 0.1, 'optimizer': 'adam'}\n",
      "8348/8348 [==============================] - 10s 1ms/step - loss: 0.0501 - accuracy: 0.9900\n",
      "2087/2087 [==============================] - 2s 703us/step - loss: 0.0019 - accuracy: 0.9994\n",
      "--- Starting trial: run-1\n",
      "{'num_units': 16, 'dropout': 0.1, 'optimizer': 'sgd'}\n",
      "8348/8348 [==============================] - 9s 1ms/step - loss: 0.0617 - accuracy: 0.9872\n",
      "2087/2087 [==============================] - 2s 732us/step - loss: 0.0092 - accuracy: 0.9977\n",
      "--- Starting trial: run-2\n",
      "{'num_units': 16, 'dropout': 0.2, 'optimizer': 'adam'}\n",
      "8348/8348 [==============================] - 9s 1ms/step - loss: 0.0346 - accuracy: 0.9936\n",
      "2087/2087 [==============================] - 2s 704us/step - loss: 0.0018 - accuracy: 0.9996\n",
      "--- Starting trial: run-3\n",
      "{'num_units': 16, 'dropout': 0.2, 'optimizer': 'sgd'}\n",
      "8348/8348 [==============================] - 9s 1ms/step - loss: 0.0762 - accuracy: 0.9881\n",
      "2087/2087 [==============================] - 2s 701us/step - loss: 0.0100 - accuracy: 0.9979\n",
      "--- Starting trial: run-4\n",
      "{'num_units': 32, 'dropout': 0.1, 'optimizer': 'adam'}\n",
      "8348/8348 [==============================] - 10s 1ms/step - loss: 0.0264 - accuracy: 0.9951\n",
      "2087/2087 [==============================] - 2s 702us/step - loss: 8.0466e-04 - accuracy: 0.9999\n",
      "--- Starting trial: run-5\n",
      "{'num_units': 32, 'dropout': 0.1, 'optimizer': 'sgd'}\n",
      "8348/8348 [==============================] - 9s 1ms/step - loss: 0.0366 - accuracy: 0.9933\n",
      "2087/2087 [==============================] - 2s 707us/step - loss: 0.0058 - accuracy: 0.9980\n",
      "--- Starting trial: run-6\n",
      "{'num_units': 32, 'dropout': 0.2, 'optimizer': 'adam'}\n",
      "8348/8348 [==============================] - 10s 1ms/step - loss: 0.0319 - accuracy: 0.9933\n",
      "2087/2087 [==============================] - 2s 697us/step - loss: 0.0019 - accuracy: 0.9997\n",
      "--- Starting trial: run-7\n",
      "{'num_units': 32, 'dropout': 0.2, 'optimizer': 'sgd'}\n",
      "8348/8348 [==============================] - 9s 1ms/step - loss: 0.0504 - accuracy: 0.9899\n",
      "2087/2087 [==============================] - 2s 706us/step - loss: 0.0084 - accuracy: 0.9981\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "def train_test_model(hparams):\n",
    "  model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "  ])\n",
    "  model.compile(\n",
    "      optimizer=hparams[HP_OPTIMIZER],\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['accuracy'],\n",
    "  )\n",
    "\n",
    "  model.fit(X_train, y_train, epochs=1) # Run with 1 epoch to speed things up for demo purposes\n",
    "  _, accuracy = model.evaluate(X_test, y_test)\n",
    "  return accuracy\n",
    "\n",
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "      hparams = {\n",
    "          HP_NUM_UNITS: num_units,\n",
    "          HP_DROPOUT: dropout_rate,\n",
    "          HP_OPTIMIZER: optimizer,\n",
    "      }\n",
    "      run_name = \"run-%d\" % session_num\n",
    "      print('--- Starting trial: %s' % run_name)\n",
    "      print({h.name: hparams[h] for h in hparams})\n",
    "      run('logs/hparam_tuning/' + run_name, hparams)\n",
    "      session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dd29866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b7a270c9c62eb39\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b7a270c9c62eb39\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36231934",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5298e31",
   "metadata": {},
   "source": [
    "# Part 2 - Building the RNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LSTM, Dropout, Input\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "hidden_nodes = int(2/3 * (X_train.shape[1] + 1))\n",
    "#print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the RNN\n",
    "def create_model():\n",
    "    #Initializing the RNN\n",
    "    model = Sequential()\n",
    "    #Adding the first LSTM layer and dropout \n",
    "    model.add(LSTM(units=21, return_sequences=False, input_shape=(30, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    #Adding a second LSTM Layer and dropout\n",
    "    #model.add(LSTM(units=20, return_sequences=False))\n",
    "    #model.add(Dropout(0.2))\n",
    "    #Adding the output layer\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "    #Output is two classes with only one present at a time. \n",
    "    #Softmax allows the model to interpret the outputs as probabilities\n",
    "\n",
    "    #Compiling the model without training\n",
    "    #Loss function is chosen together with activation. Softmax points us to binary crossentropy\n",
    "    #since we are faced with a binary classification problem\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    opt = SGD(lr=0.01)\n",
    "\n",
    "    model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0ed91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "268/268 [==============================] - 11s 35ms/step - loss: 0.2955 - acc: 0.8650\n",
      "Epoch 2/20\n",
      "268/268 [==============================] - 9s 34ms/step - loss: 0.1444 - acc: 0.9482\n",
      "Epoch 3/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.1083 - acc: 0.9638\n",
      "Epoch 4/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0934 - acc: 0.9733\n",
      "Epoch 5/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0811 - acc: 0.9786\n",
      "Epoch 6/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0731 - acc: 0.9816\n",
      "Epoch 7/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0690 - acc: 0.9827\n",
      "Epoch 8/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0601 - acc: 0.9847\n",
      "Epoch 9/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0514 - acc: 0.9864\n",
      "Epoch 10/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0395 - acc: 0.9892\n",
      "Epoch 11/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0322 - acc: 0.9907\n",
      "Epoch 12/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0434 - acc: 0.9866\n",
      "Epoch 13/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0247 - acc: 0.9925\n",
      "Epoch 14/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0213 - acc: 0.9936\n",
      "Epoch 15/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0412 - acc: 0.9874\n",
      "Epoch 16/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0223 - acc: 0.9930\n",
      "Epoch 17/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0184 - acc: 0.9947\n",
      "Epoch 18/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0168 - acc: 0.9954\n",
      "Epoch 19/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0160 - acc: 0.9956\n",
      "Epoch 20/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0137 - acc: 0.9965\n",
      "{'batch_size': 1000, 'epochs': 20}\n",
      "0.9978549593198338\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "268/268 [==============================] - 10s 33ms/step - loss: 0.2756 - acc: 0.8880\n",
      "Epoch 2/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.1325 - acc: 0.9470\n",
      "Epoch 3/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0901 - acc: 0.9712\n",
      "Epoch 4/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0661 - acc: 0.9815\n",
      "Epoch 5/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0564 - acc: 0.9843\n",
      "Epoch 6/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0482 - acc: 0.9868: 0s - loss: 0.048\n",
      "Epoch 7/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0422 - acc: 0.9887\n",
      "Epoch 8/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0357 - acc: 0.9906\n",
      "Epoch 9/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0286 - acc: 0.9926: 1s - loss: 0.0299 - acc: 0.992 - ETA: 1s -\n",
      "Epoch 10/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0202 - acc: 0.9950\n",
      "Epoch 11/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0142 - acc: 0.9971\n",
      "Epoch 12/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0119 - acc: 0.9977\n",
      "Epoch 13/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0104 - acc: 0.9980\n",
      "Epoch 14/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0095 - acc: 0.9981\n",
      "Epoch 15/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0084 - acc: 0.9983\n",
      "Epoch 16/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0078 - acc: 0.9984\n",
      "Epoch 17/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0070 - acc: 0.9985\n",
      "Epoch 18/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0068 - acc: 0.9987\n",
      "Epoch 19/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0059 - acc: 0.9988\n",
      "Epoch 20/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0057 - acc: 0.9989\n",
      "{'batch_size': 1000, 'epochs': 20}\n",
      "0.9983977696263271\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "268/268 [==============================] - 11s 33ms/step - loss: 0.2894 - acc: 0.8778 \n",
      "Epoch 2/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.1487 - acc: 0.9409\n",
      "Epoch 3/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0870 - acc: 0.9706\n",
      "Epoch 4/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0667 - acc: 0.9773\n",
      "Epoch 5/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0558 - acc: 0.9820\n",
      "Epoch 6/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0493 - acc: 0.9845\n",
      "Epoch 7/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0417 - acc: 0.9866\n",
      "Epoch 8/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0367 - acc: 0.9880\n",
      "Epoch 9/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0305 - acc: 0.9902\n",
      "Epoch 10/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0261 - acc: 0.9919\n",
      "Epoch 11/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0221 - acc: 0.9934\n",
      "Epoch 12/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0185 - acc: 0.9952: 0s - loss: 0.0186 - \n",
      "Epoch 13/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0156 - acc: 0.9966: \n",
      "Epoch 14/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0140 - acc: 0.9972\n",
      "Epoch 15/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0116 - acc: 0.9976: 3s -  - ETA: 1s -\n",
      "Epoch 16/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0110 - acc: 0.9978\n",
      "Epoch 17/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0107 - acc: 0.9979\n",
      "Epoch 18/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0096 - acc: 0.9981\n",
      "Epoch 19/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0100 - acc: 0.9980\n",
      "Epoch 20/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0085 - acc: 0.9984\n",
      "{'batch_size': 1000, 'epochs': 20}\n",
      "0.9953318299907817\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "268/268 [==============================] - 10s 33ms/step - loss: 0.2734 - acc: 0.8954\n",
      "Epoch 2/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.1332 - acc: 0.9469: 2s - loss: 0.1417 - ac - ETA\n",
      "Epoch 3/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0906 - acc: 0.9708\n",
      "Epoch 4/20\n",
      "268/268 [==============================] - 9s 34ms/step - loss: 0.0652 - acc: 0.9805\n",
      "Epoch 5/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0539 - acc: 0.9841\n",
      "Epoch 6/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0418 - acc: 0.9877\n",
      "Epoch 7/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0245 - acc: 0.9935\n",
      "Epoch 8/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0172 - acc: 0.9963\n",
      "Epoch 9/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0139 - acc: 0.9974: 0s - loss: 0.0139 - a\n",
      "Epoch 10/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0123 - acc: 0.9978\n",
      "Epoch 11/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0111 - acc: 0.9982\n",
      "Epoch 12/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0103 - acc: 0.9984\n",
      "Epoch 13/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0098 - acc: 0.9984\n",
      "Epoch 14/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0096 - acc: 0.9984: 0s - loss: 0.0096 - acc: 0. - ETA: 0s - loss: 0.0095 - acc: 0.\n",
      "Epoch 15/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0089 - acc: 0.9985\n",
      "Epoch 16/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0086 - acc: 0.9985: 0s - loss: 0.0085 - \n",
      "Epoch 17/20\n",
      "268/268 [==============================] - 9s 34ms/step - loss: 0.0080 - acc: 0.9987: 0s - loss: 0.008\n",
      "Epoch 18/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0076 - acc: 0.9987\n",
      "Epoch 19/20\n",
      "268/268 [==============================] - 9s 33ms/step - loss: 0.0072 - acc: 0.9988\n",
      "Epoch 20/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0411 - acc: 0.9939\n",
      "{'batch_size': 1000, 'epochs': 20}\n",
      "0.9981544400950565\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "268/268 [==============================] - 10s 33ms/step - loss: 0.2747 - acc: 0.8950\n",
      "Epoch 2/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.1444 - acc: 0.9489\n",
      "Epoch 3/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.1006 - acc: 0.9647\n",
      "Epoch 4/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0625 - acc: 0.9831\n",
      "Epoch 5/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0512 - acc: 0.9857\n",
      "Epoch 6/20\n",
      "268/268 [==============================] - 9s 32ms/step - loss: 0.0453 - acc: 0.9867\n",
      "Epoch 7/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0396 - acc: 0.9885: \n",
      "Epoch 8/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0334 - acc: 0.9910\n",
      "Epoch 9/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0241 - acc: 0.9930\n",
      "Epoch 10/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0192 - acc: 0.9955\n",
      "Epoch 11/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0154 - acc: 0.9968\n",
      "Epoch 12/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0132 - acc: 0.9975\n",
      "Epoch 13/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0132 - acc: 0.9974\n",
      "Epoch 14/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0115 - acc: 0.9977: 0s - loss: 0.0118 - acc\n",
      "Epoch 15/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0103 - acc: 0.9979\n",
      "Epoch 16/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0099 - acc: 0.9980\n",
      "Epoch 17/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0093 - acc: 0.9980\n",
      "Epoch 18/20\n",
      "268/268 [==============================] - 8s 31ms/step - loss: 0.0088 - acc: 0.9982\n",
      "Epoch 19/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0078 - acc: 0.9983\n",
      "Epoch 20/20\n",
      "268/268 [==============================] - 8s 32ms/step - loss: 0.0077 - acc: 0.9983\n",
      "{'batch_size': 1000, 'epochs': 20}\n",
      "0.9981432088372564\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LSTM, Dropout, Input\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "dfInterno=pd.DataFrame()\n",
    "\n",
    "hidden_nodes = int(2/3 * (X.shape[1] + 1))\n",
    "#print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the RNN\n",
    "def create_model():\n",
    "    #Initializing the RNN\n",
    "    model = Sequential()\n",
    "    #Adding the first LSTM layer and dropout \n",
    "    model.add(LSTM(units=21, return_sequences=False, input_shape=(30, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    #Adding the output layer\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "    #Output is two classes with only one present at a time. \n",
    "    #Softmax allows the model to interpret the outputs as probabilities\n",
    "\n",
    "    #Compiling the model without training\n",
    "    #Loss function is chosen together with activation. Softmax points us to binary crossentropy\n",
    "    #since we are faced with a binary classification problem\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    opt = SGD(lr=0.01)\n",
    "\n",
    "    model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [1000,5000]\n",
    "epochs = [10, 20]\n",
    "\n",
    "p_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=-1, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    \n",
    "    pipe= make_pipeline(MinMaxScaler(),StandardScaler(),grid)\n",
    "    fittedgrid=pipe.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0d00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\RNNGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d322e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "251/251 [==============================] - 12s 41ms/step - loss: 0.4980 - acc: 0.8125 - val_loss: 0.4541 - val_acc: 0.8182\n",
      "Epoch 2/20\n",
      "251/251 [==============================] - 10s 40ms/step - loss: 0.2549 - acc: 0.9093 - val_loss: 0.2017 - val_acc: 0.9394\n",
      "Epoch 3/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.1971 - acc: 0.9440 - val_loss: 0.1840 - val_acc: 0.9509\n",
      "Epoch 4/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.1824 - acc: 0.9497 - val_loss: 0.1709 - val_acc: 0.9518\n",
      "Epoch 5/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.1689 - acc: 0.9520 - val_loss: 0.1577 - val_acc: 0.9506\n",
      "Epoch 6/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.1545 - acc: 0.9526 - val_loss: 0.1456 - val_acc: 0.9532\n",
      "Epoch 7/20\n",
      "251/251 [==============================] - 10s 40ms/step - loss: 0.1560 - acc: 0.9493 - val_loss: 0.1284 - val_acc: 0.9550\n",
      "Epoch 8/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.1530 - acc: 0.9492 - val_loss: 0.1316 - val_acc: 0.9546\n",
      "Epoch 9/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.1275 - acc: 0.9568 - val_loss: 0.1076 - val_acc: 0.9618\n",
      "Epoch 10/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.1140 - acc: 0.9601 - val_loss: 0.1061 - val_acc: 0.9622\n",
      "Epoch 11/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.1094 - acc: 0.9607 - val_loss: 0.2284 - val_acc: 0.8932\n",
      "Epoch 12/20\n",
      "251/251 [==============================] - 10s 40ms/step - loss: 0.1035 - acc: 0.9627 - val_loss: 0.0783 - val_acc: 0.9681\n",
      "Epoch 13/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.0902 - acc: 0.9673 - val_loss: 0.0667 - val_acc: 0.9718\n",
      "Epoch 14/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.0928 - acc: 0.9669 - val_loss: 0.0815 - val_acc: 0.9681\n",
      "Epoch 15/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.0655 - acc: 0.9771 - val_loss: 0.0577 - val_acc: 0.9828\n",
      "Epoch 16/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.0661 - acc: 0.9788 - val_loss: 0.0442 - val_acc: 0.9873\n",
      "Epoch 17/20\n",
      "251/251 [==============================] - 10s 40ms/step - loss: 0.0927 - acc: 0.9720 - val_loss: 0.0427 - val_acc: 0.9880\n",
      "Epoch 18/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.0470 - acc: 0.9864 - val_loss: 0.0345 - val_acc: 0.9891\n",
      "Epoch 19/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.0411 - acc: 0.9879 - val_loss: 0.0317 - val_acc: 0.9894\n",
      "Epoch 20/20\n",
      "251/251 [==============================] - 10s 39ms/step - loss: 0.0362 - acc: 0.9895 - val_loss: 0.0256 - val_acc: 0.9897\n",
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     45521\n",
      "           1       0.99      1.00      0.99    204911\n",
      "\n",
      "    accuracy                           0.99    250432\n",
      "   macro avg       0.99      0.98      0.98    250432\n",
      "weighted avg       0.99      0.99      0.99    250432\n",
      "\n",
      "Accuracy Score :  0.9897736710963455\n",
      "Precision Score :  0.991656299809509\n",
      "Recall Score :  0.9958811386406782\n",
      "F1 Score :  0.9937642289290105\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     15174\n",
      "           1       0.99      1.00      0.99     68304\n",
      "\n",
      "    accuracy                           0.99     83478\n",
      "   macro avg       0.99      0.98      0.98     83478\n",
      "weighted avg       0.99      0.99      0.99     83478\n",
      "\n",
      "Accuracy Score :  0.9896739260643523\n",
      "Precision Score :  0.9913019406725334\n",
      "Recall Score :  0.9961202857812134\n",
      "F1 Score :  0.9937052723820651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "def create_model():\n",
    "    #Initializing the RNN\n",
    "    model = Sequential()\n",
    "    #Adding the first LSTM layer and dropout \n",
    "    model.add(LSTM(units=21, return_sequences=False, input_shape=(30, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    #Adding the output layer\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "    #Output is two classes with only one present at a time. \n",
    "    #Softmax allows the model to interpret the outputs as probabilities\n",
    "\n",
    "    #Compiling the model without training\n",
    "    #Loss function is chosen together with activation. Softmax points us to binary crossentropy\n",
    "    #since we are faced with a binary classification problem\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    opt = SGD(lr=0.01)\n",
    "\n",
    "    model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "X_train=MinMaxScaler().fit_transform(X_train)\n",
    "X_train=MinMaxScaler().fit_transform(X_train)\n",
    "\n",
    "X_test=MinMaxScaler().fit_transform(X_test)\n",
    "X_test=MinMaxScaler().fit_transform(X_test)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=20, validation_data=(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \",f1_score(y_train,y_pred,average='binary'))\n",
    "#################################################################\n",
    "y_pred = model.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score : \", accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \", f1_score(y_test,y_pred,average='binary'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82339a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "907d122e",
   "metadata": {},
   "source": [
    "# Trying it out on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe61e1",
   "metadata": {},
   "source": [
    "X = dfO.copy()\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "# Part 2 - Building the RNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "\n",
    "hidden_nodes = int(2/3 * (X_train.shape[1] + 1))\n",
    "#print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the RNN\n",
    "def create_model():\n",
    "#Enc Dimension \n",
    "    encoding_dim=100\n",
    "    #Input shape\n",
    "    input_dim = Input(shape=(61,))\n",
    "    #Encoding Layer\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_dim)\n",
    "    #Decoding Layer\n",
    "    decoded = Dense(1, activation='sigmoid')(encoded)\n",
    "\n",
    "    #Model AE\n",
    "    autoencoder = Model(input_dim, decoded)\n",
    "    #Model Encoder \n",
    "    encoder = Model(input_dim, encoded)\n",
    "    #Encoding\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    #Decoding \n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    #Model Decoder \n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "    #optimizer=tf.keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=None, decay=0.0)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model = autoencoder\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f864eea6",
   "metadata": {},
   "source": [
    "pd.DataFrame(grid.cv_results_).to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\WholeGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38467c5",
   "metadata": {},
   "source": [
    "# Building the CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf88276",
   "metadata": {},
   "source": [
    "#Building the CNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "X = df.copy()\n",
    "#X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "#Adding the extra dimension CNN needs\n",
    "SX_train=StandardScaler().fit_transform(X_train)\n",
    "\n",
    "X_train=X_train.to_numpy()\n",
    "X_train = SX_train[..., np.newaxis]\n",
    "X_test=X_test.to_numpy()\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the CNN\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=1, kernel_size=3, activation='relu'))\n",
    "    #model.add(Conv1D(filters=31, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=\"sigmoid\")) # binary activation output\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427076c",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "#print(results_nm)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label='positive',\n",
    "                                           average='micro'))\n",
    "print(f1_score(y_test,y_pred,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e52825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "8348/8348 [==============================] - 15s 2ms/step - loss: 0.0525 - accuracy: 0.9827\n",
      "Epoch 2/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0208 - accuracy: 0.9949\n",
      "Epoch 3/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0189 - accuracy: 0.9956\n",
      "Epoch 4/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0182 - accuracy: 0.9958\n",
      "Epoch 5/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0169 - accuracy: 0.9961\n",
      "Epoch 6/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0163 - accuracy: 0.9963\n",
      "Epoch 7/20\n",
      "8348/8348 [==============================] - 15s 2ms/step - loss: 0.0167 - accuracy: 0.9961\n",
      "Epoch 8/20\n",
      "8348/8348 [==============================] - 15s 2ms/step - loss: 0.0163 - accuracy: 0.9963\n",
      "Epoch 9/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0160 - accuracy: 0.9964\n",
      "Epoch 10/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0160 - accuracy: 0.9964\n",
      "Epoch 11/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0154 - accuracy: 0.9966\n",
      "Epoch 12/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0149 - accuracy: 0.9966\n",
      "Epoch 13/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0148 - accuracy: 0.9966\n",
      "Epoch 14/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0148 - accuracy: 0.9966\n",
      "Epoch 15/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0142 - accuracy: 0.9967\n",
      "Epoch 16/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0140 - accuracy: 0.9967\n",
      "Epoch 17/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0141 - accuracy: 0.9966\n",
      "Epoch 18/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0134 - accuracy: 0.9968\n",
      "Epoch 19/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0131 - accuracy: 0.9968\n",
      "Epoch 20/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0129 - accuracy: 0.9968\n",
      "{'batch_size': 32, 'epochs': 20}\n",
      "0.9985624869395882\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0684 - accuracy: 0.9762\n",
      "Epoch 2/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0216 - accuracy: 0.9943\n",
      "Epoch 3/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0186 - accuracy: 0.9957\n",
      "Epoch 4/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0171 - accuracy: 0.9959\n",
      "Epoch 5/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0161 - accuracy: 0.9964\n",
      "Epoch 6/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0153 - accuracy: 0.9965\n",
      "Epoch 7/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0147 - accuracy: 0.9967\n",
      "Epoch 8/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0144 - accuracy: 0.9966\n",
      "Epoch 9/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0138 - accuracy: 0.9968\n",
      "Epoch 10/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0138 - accuracy: 0.9967\n",
      "Epoch 11/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0138 - accuracy: 0.9967\n",
      "Epoch 12/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0130 - accuracy: 0.9970\n",
      "Epoch 13/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0131 - accuracy: 0.9969\n",
      "Epoch 14/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0129 - accuracy: 0.9970\n",
      "Epoch 15/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0131 - accuracy: 0.9970\n",
      "Epoch 16/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0125 - accuracy: 0.9970\n",
      "Epoch 17/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0123 - accuracy: 0.9971\n",
      "Epoch 18/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0121 - accuracy: 0.9972\n",
      "Epoch 19/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0114 - accuracy: 0.9972\n",
      "Epoch 20/20\n",
      "4174/4174 [==============================] - 10s 2ms/step - loss: 0.0116 - accuracy: 0.9973\n",
      "{'batch_size': 64, 'epochs': 20}\n",
      "0.9985662295944231\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/10\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0523 - accuracy: 0.9824\n",
      "Epoch 2/10\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0191 - accuracy: 0.9954\n",
      "Epoch 3/10\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0174 - accuracy: 0.9960\n",
      "Epoch 4/10\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0165 - accuracy: 0.9959\n",
      "Epoch 5/10\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0162 - accuracy: 0.9962\n",
      "Epoch 6/10\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0154 - accuracy: 0.9964\n",
      "Epoch 7/10\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0152 - accuracy: 0.9965\n",
      "Epoch 8/10\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0154 - accuracy: 0.9964\n",
      "Epoch 9/10\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0153 - accuracy: 0.9964\n",
      "Epoch 10/10\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0149 - accuracy: 0.9964\n",
      "{'batch_size': 32, 'epochs': 10}\n",
      "0.998498845200778\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0570 - accuracy: 0.9801\n",
      "Epoch 2/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0248 - accuracy: 0.9931\n",
      "Epoch 3/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0222 - accuracy: 0.9940\n",
      "Epoch 4/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0208 - accuracy: 0.9946\n",
      "Epoch 5/20\n",
      "8348/8348 [==============================] - 14s 2ms/step - loss: 0.0200 - accuracy: 0.9948\n",
      "Epoch 6/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0196 - accuracy: 0.9950\n",
      "Epoch 7/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0189 - accuracy: 0.9953\n",
      "Epoch 8/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0181 - accuracy: 0.9954\n",
      "Epoch 9/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0183 - accuracy: 0.9952\n",
      "Epoch 10/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0174 - accuracy: 0.9957\n",
      "Epoch 11/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0164 - accuracy: 0.9958\n",
      "Epoch 12/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0163 - accuracy: 0.9959\n",
      "Epoch 13/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0165 - accuracy: 0.9957\n",
      "Epoch 14/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0159 - accuracy: 0.9959\n",
      "Epoch 15/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0155 - accuracy: 0.9961\n",
      "Epoch 16/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0150 - accuracy: 0.9963\n",
      "Epoch 17/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0147 - accuracy: 0.9964\n",
      "Epoch 18/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0141 - accuracy: 0.9965\n",
      "Epoch 19/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0133 - accuracy: 0.9967\n",
      "Epoch 20/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0134 - accuracy: 0.9967\n",
      "{'batch_size': 32, 'epochs': 20}\n",
      "0.9986448437043258\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0497 - accuracy: 0.9832\n",
      "Epoch 2/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0192 - accuracy: 0.9955\n",
      "Epoch 3/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0180 - accuracy: 0.9960\n",
      "Epoch 4/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0175 - accuracy: 0.9961\n",
      "Epoch 5/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0165 - accuracy: 0.9964\n",
      "Epoch 6/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0156 - accuracy: 0.9966\n",
      "Epoch 7/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0155 - accuracy: 0.9964\n",
      "Epoch 8/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0153 - accuracy: 0.9967\n",
      "Epoch 9/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0153 - accuracy: 0.9965\n",
      "Epoch 10/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0150 - accuracy: 0.9967\n",
      "Epoch 11/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0147 - accuracy: 0.9968\n",
      "Epoch 12/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0144 - accuracy: 0.9966\n",
      "Epoch 13/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0145 - accuracy: 0.9968\n",
      "Epoch 14/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0144 - accuracy: 0.9968\n",
      "Epoch 15/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0145 - accuracy: 0.9968\n",
      "Epoch 16/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0147 - accuracy: 0.9967\n",
      "Epoch 17/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0146 - accuracy: 0.9966\n",
      "Epoch 18/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0146 - accuracy: 0.9967\n",
      "Epoch 19/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0143 - accuracy: 0.9967\n",
      "Epoch 20/20\n",
      "8348/8348 [==============================] - 13s 2ms/step - loss: 0.0147 - accuracy: 0.9965\n",
      "{'batch_size': 32, 'epochs': 20}\n",
      "0.9986336141982786\n"
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the CNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the CNN\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=31, kernel_size=3, activation='relu'))\n",
    "    #model.add(Conv1D(filters=31, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=\"sigmoid\")) # binary activation output\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [32, 64]\n",
    "#optimizer= [\"RMSprop\", \"adam\"]\n",
    "epochs = [10, 20]\n",
    "#dropout = [0, 0.1, 0.2]\n",
    "p_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "scoring = {'accuracy', \"precision\", \"recall\",  \"f1\"}\n",
    "dfInterno=pd.DataFrame()\n",
    "\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Nested CV with parameter optimization\n",
    "for train_index, test_index in outer_cv.split(X,y):\n",
    "\n",
    "    X_train,X_test= X.iloc[train_index,:], X.iloc[test_index,:] \n",
    "    y_train,y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    X_train=MinMaxScaler().fit_transform(X_train)\n",
    "    X_train=MinMaxScaler().fit_transform(X_train)\n",
    "\n",
    "    X_test=MinMaxScaler().fit_transform(X_test)\n",
    "    X_test=MinMaxScaler().fit_transform(X_test)\n",
    "\n",
    "    #X_train=X_train.to_numpy()\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    #X_test=X_test.to_numpy()\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    \n",
    "    grid = GridSearchCV(model,return_train_score=True,scoring=scoring, refit=\"accuracy\",param_grid=p_grid, n_jobs=-1, cv= StratifiedKFold(n_splits=5, shuffle=True, random_state=1), verbose=4)\n",
    "    fittedgrid=grid.fit(X_train, y_train)\n",
    "#Saving the 25 subresults\n",
    "    dfCurrent= pd.DataFrame(grid.cv_results_)\n",
    "    dfInterno = pd.concat([dfInterno, dfCurrent], axis=0)\n",
    "#Saving the best params and the results\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    #dfBest= pd.concat([dfBest, dfCurrentBest], axis=0)      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8396a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterno.to_csv(r\"C:\\Users\\White\\Desktop\\tfg\\generados\\CNNGridSearchCV.csv\", index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3559e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7826/7826 [==============================] - 15s 2ms/step - loss: 0.0535 - accuracy: 0.9822 - val_loss: 0.0139 - val_accuracy: 0.9981\n",
      "Epoch 2/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0199 - accuracy: 0.9949 - val_loss: 0.0105 - val_accuracy: 0.9982\n",
      "Epoch 3/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0175 - accuracy: 0.9957 - val_loss: 0.0089 - val_accuracy: 0.9984\n",
      "Epoch 4/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0161 - accuracy: 0.9959 - val_loss: 0.0077 - val_accuracy: 0.9986\n",
      "Epoch 5/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0155 - accuracy: 0.9962 - val_loss: 0.0073 - val_accuracy: 0.9986\n",
      "Epoch 6/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0151 - accuracy: 0.9962 - val_loss: 0.0071 - val_accuracy: 0.9984\n",
      "Epoch 7/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0148 - accuracy: 0.9964 - val_loss: 0.0076 - val_accuracy: 0.9988\n",
      "Epoch 8/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0144 - accuracy: 0.9964 - val_loss: 0.0067 - val_accuracy: 0.9986\n",
      "Epoch 9/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0140 - accuracy: 0.9964 - val_loss: 0.0065 - val_accuracy: 0.9988\n",
      "Epoch 10/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0136 - accuracy: 0.9967 - val_loss: 0.0063 - val_accuracy: 0.9987\n",
      "Epoch 11/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0136 - accuracy: 0.9967 - val_loss: 0.0063 - val_accuracy: 0.9988\n",
      "Epoch 12/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0132 - accuracy: 0.9968 - val_loss: 0.0061 - val_accuracy: 0.9988\n",
      "Epoch 13/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0128 - accuracy: 0.9968 - val_loss: 0.0057 - val_accuracy: 0.9989\n",
      "Epoch 14/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0130 - accuracy: 0.9968 - val_loss: 0.0053 - val_accuracy: 0.9990\n",
      "Epoch 15/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0124 - accuracy: 0.9970 - val_loss: 0.0055 - val_accuracy: 0.9990\n",
      "Epoch 16/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0116 - accuracy: 0.9971 - val_loss: 0.0056 - val_accuracy: 0.9991\n",
      "Epoch 17/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0117 - accuracy: 0.9972 - val_loss: 0.0052 - val_accuracy: 0.9991\n",
      "Epoch 18/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0113 - accuracy: 0.9973 - val_loss: 0.0049 - val_accuracy: 0.9990\n",
      "Epoch 19/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0108 - accuracy: 0.9974 - val_loss: 0.0047 - val_accuracy: 0.9991\n",
      "Epoch 20/20\n",
      "7826/7826 [==============================] - 14s 2ms/step - loss: 0.0110 - accuracy: 0.9974 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
      "results for train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45521\n",
      "           1       1.00      1.00      1.00    204911\n",
      "\n",
      "    accuracy                           1.00    250432\n",
      "   macro avg       1.00      1.00      1.00    250432\n",
      "weighted avg       1.00      1.00      1.00    250432\n",
      "\n",
      "Accuracy Score :  0.9989857526194735\n",
      "Precision Score :  0.9989273368211139\n",
      "Recall Score :  0.9998340743054301\n",
      "F1 Score :  0.999380499892685\n",
      "---------------------------------------------------\n",
      "results for test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15174\n",
      "           1       1.00      1.00      1.00     68304\n",
      "\n",
      "    accuracy                           1.00     83478\n",
      "   macro avg       1.00      1.00      1.00     83478\n",
      "weighted avg       1.00      1.00      1.00     83478\n",
      "\n",
      "Accuracy Score :  0.9991854141210857\n",
      "Precision Score :  0.9991660570592539\n",
      "Recall Score :  0.9998389552588428\n",
      "F1 Score :  0.9995023929047083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X = df.copy()\n",
    "X[\"LabelA\"]=df2[\"LabelA\"]\n",
    "y = X.pop(\"LabelA\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "# Initialising the CNN\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=31, kernel_size=3, activation='relu'))\n",
    "    #model.add(Conv1D(filters=31, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=\"sigmoid\")) # binary activation output\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "##############################################################\n",
    "#Test the best parameters on all the set\n",
    "X_train=MinMaxScaler().fit_transform(X_train)\n",
    "X_train=MinMaxScaler().fit_transform(X_train)\n",
    "X_test=MinMaxScaler().fit_transform(X_test)\n",
    "X_test=MinMaxScaler().fit_transform(X_test)\n",
    "\n",
    "#X_train=X_train.to_numpy()\n",
    "X_train = X_train[..., np.newaxis]\n",
    "#X_test=X_test.to_numpy()\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test))\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "#-----------------------------------------\n",
    "results_nm = confusion_matrix(y_train,y_pred)\n",
    "print(\"results for train set\")\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(\"Accuracy Score : \",accuracy_score(y_train,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_train,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \",f1_score(y_train,y_pred,average='binary'))\n",
    "#################################################################\n",
    "y_pred = model.predict(X_test)\n",
    "#---------------------------------------------------\n",
    "results_nm = confusion_matrix(y_test,y_pred)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"results for test set\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy Score : \", accuracy_score(y_test,y_pred))\n",
    "print(\"Precision Score : \",precision_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"Recall Score : \",recall_score(y_test,y_pred, \n",
    "                                           pos_label=1,\n",
    "                                           average='binary'))\n",
    "print(\"F1 Score : \", f1_score(y_test,y_pred,average='binary'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f8b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
